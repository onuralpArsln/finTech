{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/onuralpArsln/finTech/blob/main/tryOtherMethods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering you already have your files inside github links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Güncel data linkleri <br> <br>\n",
    "\n",
    "file1 = \"https://raw.githubusercontent.com/onuralpArsln/finTech/refs/heads/main/data/lastWeek.csv?token=GHSAT0AAAAAACLQZ7SITTNWYGU4AFMRQCZ2ZXPXZZA\"\n",
    "file2 = \"https://raw.githubusercontent.com/onuralpArsln/finTech/refs/heads/main/data/twoWeek.csv?token=GHSAT0AAAAAACLQZ7SIAZ6M2GER76YGBQ4EZXPXZ3A\"\n",
    "file3 = \"https://raw.githubusercontent.com/onuralpArsln/finTech/refs/heads/main/data/fourWeek.csv?token=GHSAT0AAAAAACLQZ7SIZUNDS5LREKVI4BA4ZXPXY2Q\"\n",
    "\n",
    "allLinks =[file1, file2, file3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data into arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "series_list=[]\n",
    "for url in allLinks:\n",
    "  df= pd.read_csv(url, header=None)\n",
    "  data=df.values.tolist()\n",
    "  single_dimensional_list = [item[0] for item in data]\n",
    "  series_list.append( single_dimensional_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dfs=[]\n",
    "train_dfs=[]\n",
    "\n",
    "for series in series_list:\n",
    " \n",
    "    last_10 = series[-10:]\n",
    "  \n",
    "    rest = series[:-10]\n",
    "    # append to dataset\n",
    "    test_dfs.append(last_10)\n",
    "    train_dfs.append(rest)\n",
    "\n",
    "print(test_dfs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buradan aşağısı chat gpt copy8 pastesi ondan düzeltme lazım \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LSTM (Long Short-Term Memory) Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Convert train_dfs to a DataFrame\n",
    "prices = pd.DataFrame(train_dfs, columns=['Close'])\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_prices = scaler.fit_transform(prices)\n",
    "\n",
    "# Create sequences\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i:(i + time_step), 0]\n",
    "        X.append(a)\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_dataset(scaled_prices, time_step=10)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train model\n",
    "model.fit(X, y, epochs=50, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "predictions = scaler.inverse_transform(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. GARCH (Generalized Autoregressive Conditional Heteroskedasticity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from arch import arch_model\n",
    "\n",
    "# Convert train_dfs to a DataFrame\n",
    "prices = pd.DataFrame(train_dfs, columns=['Close'])\n",
    "returns = prices['Close'].pct_change().dropna()\n",
    "\n",
    "# Fit GARCH model\n",
    "model = arch_model(returns, vol='Garch', p=1, q=1)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make volatility predictions\n",
    "predicted_volatility = model_fit.conditional_volatility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Convert train_dfs to a DataFrame\n",
    "prices = pd.DataFrame(train_dfs, columns=['Close'])\n",
    "prices['Lag1'] = prices['Close'].shift(1)  # Create lagged feature\n",
    "prices = prices.dropna()\n",
    "\n",
    "# Prepare features and target\n",
    "X = prices[['Lag1']]\n",
    "y = prices['Close']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
